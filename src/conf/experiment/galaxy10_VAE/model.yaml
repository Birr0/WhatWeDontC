model:
  _target_: wwdc.models.galaxy10.VAE

lightning_loader: # keep as default (populate the above with the correct config)
  _target_: wwdc.training.lightning_loaders.LightningVAE
  vae: ${model}
  lr: 0.0001
  batch_size: ${data.loader.batch_size}
  beta: 0.000001
  vae_ckpt_path: null

model_checkpoint:
  _target_: lightning.pytorch.callbacks.ModelCheckpoint
  dirpath: ${paths.experiment_path}/ckpts/
  save_top_k: 1
  filename: ${hydra:job.id}
  monitor: val_loss
  mode: min

early_stopping:
  _target_: lightning.pytorch.callbacks.EarlyStopping
  monitor: val_loss
  patience: 100
  min_delta: 0.0001
  mode: min

trainer:
  logger:
    - ${logger.wandb}
    - ${logger.csv}
  callbacks:
    - ${...model_checkpoint}
    - ${...early_stopping}

trainer_ckpt_path: null # resume training from this checkpoint if avaialble
