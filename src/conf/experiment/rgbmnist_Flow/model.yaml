latent_dim: 64
lightning_loader:
  _target_: wwdc.models.rgbmnist.LightningFlowMatching
  vae:
    _target_: wwdc.models.rgbmnist.VAE
    hidden_dim: ${latent_dim}
  lr: 0.0003
  size: ${latent_dim}
  hidden_dim: 256
  catalog: ${data.y_catalog}
  batch_size: ${data.loader.batch_size}
  vae_ckpt_path: ${meta.vae_ckpt_path}

model_checkpoint:
  _target_: lightning.pytorch.callbacks.ModelCheckpoint
  dirpath: ${paths.experiment_path}/ckpts/
  save_top_k: 1
  filename: ${hydra:job.id}
  monitor: val_loss
  mode: min

early_stopping:
  _target_: lightning.pytorch.callbacks.EarlyStopping
  monitor: val_loss
  patience: 100
  min_delta: 0.00001
  mode: min

trainer:
  logger:
    - ${logger.wandb}
    - ${logger.csv}

  callbacks:
    - ${...model_checkpoint}
    - ${...early_stopping}

trainer_ckpt_path: null #Â resume training from this checkpoint if avaialble
