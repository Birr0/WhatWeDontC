latent_dim: 1 # not required
lightning_loader:
  _target_: wwdc.models.galaxy10.LightningFlowMatching
  vae:
    _target_: wwdc.models.galaxy10.VAE
  lr: 0.0001
  size: ${latent_dim}
  hidden_dim: 256
  catalog: ${data.y_catalog}
  batch_size: ${data.loader.batch_size}
  vae_ckpt_path: ${meta.vae_ckpt_path}

model_checkpoint: # this can be more general. Import from checkpoints and change monitor.
  _target_: lightning.pytorch.callbacks.ModelCheckpoint
  dirpath: ${paths.experiment_path}/ckpts/
  save_top_k: 1
  filename: ${hydra:job.id}
  monitor: val_loss
  mode: min

early_stopping:
  _target_: lightning.pytorch.callbacks.EarlyStopping
  monitor: val_loss
  patience: 20
  min_delta: 0.01
  mode: min

trainer:
  logger:
    - ${logger.wandb}
    - ${logger.csv}

  callbacks:
    - ${...model_checkpoint}
    - ${...early_stopping}

trainer_ckpt_path: null # resume training from this checkpoint if avaialble
