model:
  _target_: wwdc.models.rgbmnist.VAE
  hidden_dim: 64

lightning_loader: # keep as default (populate the above with the correct config)
  _target_: wwdc.training.lightning_loaders.LightningVAE
  vae: ${model}
  lr: 0.0005
  batch_size: ${data.loader.batch_size}
  beta: 0.0001
  vae_ckpt_path: null

model_checkpoint:
  _target_: lightning.pytorch.callbacks.ModelCheckpoint
  dirpath: ${paths.experiment_path}/ckpts/
  save_top_k: 1
  filename: ${hydra:job.id}
  monitor: val_loss
  mode: min

trainer:
  logger:
    - ${logger.wandb}
    - ${logger.csv}
  callbacks:
    - ${...model_checkpoint}

trainer_ckpt_path: null # resume training from this checkpoint if avaialble
